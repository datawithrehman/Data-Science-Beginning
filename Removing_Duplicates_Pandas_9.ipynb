{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d53981",
   "metadata": {},
   "source": [
    "---\n",
    "##  Author Information\n",
    "\n",
    "**Name:** Abdul Rehman  \n",
    "**Role:** Data Science Enthusiast | Python Learner  \n",
    "**Notebook Created:** 22-July-2025  \n",
    "\n",
    "**Connect with Me:**  \n",
    "\n",
    "\n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-blue?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/abdul-rehman-74b418350/)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?style=flat&logo=github&logoColor=white)](https://github.com/datawithrehman/Data-Science-Beginning)\n",
    "[![Twitter](https://img.shields.io/badge/Twitter-blue?style=flat&logo=twitter&logoColor=white)](https://x.com/datawithrehman)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Removing Duplicates & Fixing Messy Data with Pandas\n",
    "Cleaning data is often more important than modeling. If your dataset is messy, even the best machine learning models will give poor results.\n",
    "\n",
    "In this notebook, we'll explore how to:\n",
    "- Find and remove duplicates\n",
    "- Clean messy text data\n",
    "- Reset the index after removing rows\n",
    "- Avoid common beginner mistakes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with duplicates and messy strings\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Alice', 'Carol', 'bob '],\n",
    "    'City': ['Lahore', 'Karachi', 'Lahore', 'Islamabad', 'karachi'],\n",
    "    'Age': [25, 30, 25, 27, 30]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Finding Duplicates\n",
    "We can find duplicate rows using `.duplicated()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Removing Duplicate Rows\n",
    "We use `.drop_duplicates()` to remove duplicate rows.\n",
    "\n",
    "‚ö†Ô∏è **Important:** Don‚Äôt forget to either use `inplace=True` or reassign the result to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.drop_duplicates()\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Fixing Messy Text Data\n",
    "We‚Äôll clean whitespace and fix capitalization issues using `.str.strip()` and `.str.lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip spaces and convert to lowercase\n",
    "df_cleaned['Name'] = df_cleaned['Name'].str.strip().str.lower()\n",
    "df_cleaned['City'] = df_cleaned['City'].str.strip().str.lower()\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Resetting the Index\n",
    "After dropping or modifying rows, reset the index using `reset_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö® Common Mistake Alert\n",
    "- Forgetting to assign the cleaned DataFrame to a new variable\n",
    "- Not using `inplace=True` when needed\n",
    "\n",
    "```python\n",
    "# Wrong:\n",
    "df.drop_duplicates()\n",
    "# This does nothing unless reassigned or inplace=True\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÜ Mini Challenge\n",
    "**Try this:** Create your own DataFrame with duplicate rows and use `drop_duplicates()` to clean it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
